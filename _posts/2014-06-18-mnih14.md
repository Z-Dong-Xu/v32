---
supplementary: http://proceedings.mlr.press/v32/mnih14-supp.pdf
title: Neural Variational Inference and Learning in Belief Networks
abstract: Highly expressive directed latent variable models, such as sigmoid belief
  networks, are difficult to train on large datasets because exact inference in them
  is intractable and none of the approximate inference methods that have been applied
  to them scale well. We propose a fast non-iterative approximate inference method
  that uses a feedforward network to implement efficient exact sampling from the variational
  posterior. The model and this inference network are trained jointly by maximizing
  a variational lower bound on the log-likelihood. Although the naive estimator of
  the inference network gradient is too high-variance to be useful, we make it practical
  by applying several straightforward model-independent variance reduction techniques.
  Applying our approach to training sigmoid belief networks and deep autoregressive
  networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves
  state-of-the-art results on the Reuters RCV1 document dataset.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mnih14
month: 0
tex_title: Neural Variational Inference and Learning in Belief Networks
firstpage: 1791
lastpage: 1799
page: 1791-1799
order: 1791
cycles: false
author:
- given: Andriy
  family: Mnih
- given: Karol
  family: Gregor
date: 2014-06-18
number: 2
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 6
  - 18
pdf: http://proceedings.mlr.press/v32/mnih14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
