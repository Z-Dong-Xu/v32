---
pdf: http://proceedings.mlr.press/v32/lib14.pdf
section: cycle-1
title: On Modelling Non-linear Topical Dependencies
abstract: Probabilistic topic models such as Latent Dirichlet Allocation (LDA) discover
  latent topics from large corpora by exploiting wordsâ€™ co-occurring relation. By
  observing the topical similarity between words, we find that some other relations,
  such as semantic or syntax relation between words, lead to strong dependence between
  their topics. In this paper, sentences are represented as dependency trees and a
  Global Topic Random Field (GTRF) is presented to model the non-linear dependencies
  between words. To infer our model, a new global factor is defined over all edges
  and the normalization factor of GRF is proven to be a constant. As a result, no
  independent assumption is needed when inferring our model. Based on it, we develop
  an efficient expectation-maximization (EM) procedure for parameter estimation. Experimental
  results on four data sets show that GTRF achieves much lower perplexity than LDA
  and linear dependency topic models and produces better topic coherence.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lib14
month: 0
firstpage: 458
lastpage: 466
page: 458-466
sections: 
author:
- given: Zhixing
  family: Li
- given: Siqiang
  family: Wen
- given: Juanzi
  family: Li
- given: Peng
  family: Zhang
- given: Jie
  family: Tang
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
