---
supplementary: Supplementary:bai14-supp.pdf
title: A Bayesian Framework for Online Classifier Ensemble
abstract: We propose a Bayesian framework for recursively estimating the classifier
  weights in online learning of a classifier ensemble. In contrast with past methods,
  such as stochastic gradient descent or online boosting, our framework estimates
  the weights in terms of evolving posterior distributions. For a specified class
  of loss functions, we show that it is possible to formulate a suitably defined likelihood
  function and hence use the posterior distribution as an approximation to the global
  empirical loss minimizer. If the stream of training data is sampled from a stationary
  process, we can also show that our framework admits a superior rate of convergence
  to the expected loss minimizer than is possible with standard stochastic gradient
  descent. In experiments with real-world datasets, our formulation often performs
  better than online boosting algorithms.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bai14
month: 0
firstpage: 1584
lastpage: 1592
page: 1584-1592
sections: 
author:
- given: Qinxun
  family: Bai
- given: Henry
  family: Lam
- given: Stan
  family: Sclaroff
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/bai14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
