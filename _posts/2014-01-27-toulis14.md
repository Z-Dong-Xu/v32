---
title: Statistical analysis of stochastic gradient methods for generalized linear
  models
abstract: 'We study the statistical properties of stochastic gradient descent (SGD)
  using   explicit and implicit updates for fitting generalized linear models (GLMs).  Initially,
  we develop a computationally   efficient algorithm to implement implicit SGD learning
  of GLMs.  Next, we obtain exact formulas for the bias and variance  of both updates
  which leads to two important observations on their   comparative statistical properties.  First,
  in small samples, the estimates from the implicit procedure   are more biased than
  the estimates from the explicit one,   but their empirical variance is smaller and
  they are more robust to   learning rate misspecification.   Second, the two procedures
  are statistically identical in the limit:   they are both unbiased, converge at
  the same rate and have the   same asymptotic variance. Our set of experiments confirm
  our theory and   more broadly suggest that the implicit procedure can be a competitive
  choice   for fitting large-scale  models, especially when robustness is a concern.'
section: cycle-2
layout: inproceedings
id: toulis14
month: 0
firstpage: 667
lastpage: 675
page: 667-675
sections: 
author:
- given: Panagiotis
  family: Toulis
- given: Edoardo
  family: Airoldi
- given: Jason
  family: Rennie
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/toulis14/toulis14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
