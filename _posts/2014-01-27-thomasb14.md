---
title: 'GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative
  Convergence Results'
abstract: Natural gradient ascent (NGA) is a popular optimization method that uses
  a positive definite metric tensor. In many applications the metric tensor is only
  guaranteed to be positive semidefinite (e.g., when using the Fisher information
  matrix as the metric tensor), in which case NGA is not applicable. In our first
  contribution, we derive generalized natural gradient ascent (GeNGA), a generalization
  of NGA which allows for positive semidefinite non-smooth metric tensors. In our
  second contribution we show that, in standard settings, GeNGA and NGA can both be
  divergent. We then establish sufficient conditions to ensure that both achieve various
  forms of convergence. In our third contribution we show how several reinforcement
  learning methods that use NGA without positive definite metric tensors can be adapted
  to properly use GeNGA.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: thomasb14
month: 0
tex_title: 'GeNGA: A Generalization of Natural Gradient Ascent with Positive and Negative
  Convergence Results'
firstpage: 1575
lastpage: 1583
page: 1575-1583
sections: 
author:
- given: Philip
  family: Thomas
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/thomasb14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
