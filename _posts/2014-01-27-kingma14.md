---
title: Efficient Gradient-Based Inference through Transformations between Bayes Nets
  and Neural Nets
abstract: Hierarchical Bayesian networks and neural networks with stochastic hidden
  units are commonly perceived as two separate types of models. We show that either
  of these types of models can often be transformed into an instance of the other,
  by switching between centered and differentiable non-centered parameterizations
  of the latent variables. The choice of parameterization greatly influences the efficiency
  of gradient-based posterior inference; we show that they are often complementary
  to eachother, we clarify when each parameterization is preferred and show how inference
  can be made robust. In the non-centered form, a simple Monte Carlo estimator of
  the marginal likelihood can be used for learning the parameters. Theoretical results
  are supported by experiments.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kingma14
month: 0
firstpage: 1782
lastpage: 1790
page: 1782-1790
sections: 
author:
- given: Diederik
  family: Kingma
- given: Max
  family: Welling
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/kingma14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
