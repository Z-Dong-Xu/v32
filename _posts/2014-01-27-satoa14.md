---
title: 'Approximation Analysis of Stochastic Gradient Langevin Dynamics  by using
  Fokker-Planck Equation and Ito Process '
abstract: The stochastic gradient Langevin dynamics (SGLD) algorithm is appealing
  for large scale Bayesian learning.  The SGLD algorithm seamlessly transit stochastic
  optimization and Bayesian posterior sampling.  However, solid theories, such as
  convergence proof, have not been developed.  We theoretically analyze the SGLD algorithm
  with constant stepsize in two ways.  First, we show  by using the Fokker-Planck
  equation that the probability distribution of random variables generated by the
  SGLD algorithm converges to the Bayesian posterior.  Second, we analyze the convergence
  of the SGLD algorithm by using the Ito process, which reveals that the SGLD algorithm
  does not strongly but weakly converges.  This result indicates that the SGLD algorithm
  can be an approximation method for posterior averaging.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: satoa14
month: 0
firstpage: 982
lastpage: 990
page: 982-990
sections: 
author:
- given: Issei
  family: Sato
- given: Hiroshi
  family: Nakagawa
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/satoa14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
