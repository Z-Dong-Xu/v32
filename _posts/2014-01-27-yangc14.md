---
supplementary: Supplementary:yangc14-supp.zip
title: Elementary Estimators for High-Dimensional Linear Regression
abstract: We consider the problem of structurally constrained high-dimensional linear
  regression. This has attracted considerable attention over the last decade, with
  state of the art statistical estimators based on solving regularized convex programs.
  While these typically non-smooth convex programs can be solved in polynomial time,
  scaling the state of the art optimization methods to very large-scale problems is
  an ongoing and rich area of research. In this paper, we attempt to address this
  scaling issue at the source, by asking whether one can build \emphsimpler possibly
  closed-form estimators, that yet come with statistical guarantees that are nonetheless
  comparable to regularized likelihood estimators! We answer this question in the
  affirmative, with variants of the classical ridge and OLS (ordinary least squares
  estimators) for linear regression. We analyze our estimators in the high-dimensional
  setting, and moreover provide empirical corroboration of its performance on simulated
  as well as real world microarray data.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yangc14
month: 0
firstpage: 388
lastpage: 396
page: 388-396
sections: 
author:
- given: Eunho
  family: Yang
- given: Aurelie
  family: Lozano
- given: Pradeep
  family: Ravikumar
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/yangc14/yangc14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
