---
title: An Asynchronous Parallel Stochastic Coordinate Descent Algorithm
abstract: We describe an asynchronous parallel stochastic coordinate descent algorithm
  for minimizing smooth unconstrained or separably constrained functions. The method
  achieves a linear convergence rate on functions that satisfy an essential strong
  convexity property and a sublinear rate (1/K) on general convex functions. Near-linear
  speedup on a multicore system can be expected if the number of processors is O(n^1/2)
  in unconstrained optimization and O(n^1/4) in the separable-constrained case, where
  n is the number of variables. We  describe results from implementation on 40-core
  processors.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liud14
month: 0
firstpage: 469
lastpage: 477
page: 469-477
sections: 
author:
- given: Ji
  family: Liu
- given: Steve
  family: Wright
- given: Christopher
  family: Re
- given: Victor
  family: Bittorf
- given: Srikrishna
  family: Sridhar
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/liud14/liud14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
