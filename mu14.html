<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery">

  <meta name="citation_author" content="Mu, Cun">

  <meta name="citation_author" content="Huang, Bo">

  <meta name="citation_author" content="Wright, John">

  <meta name="citation_author" content="Goldfarb, Donald">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="73">
<meta name="citation_lastpage" content="81">
<meta name="citation_pdf_url" content="mu14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery</h1>

	<div id="authors">
	
		Cun Mu,
	
		Bo Huang,
	
		John Wright,
	
		Donald Goldfarb
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 73â€“81, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms (SNN) of the unfolding matrices of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a <span class="math">\(K\)</span>-way <span class="math">\(n\)</span><span class="math">\(\times\)</span><span class="math">\(n\)</span><span class="math">\(\times\)</span><span class="math">\(\cdots\)</span><span class="math">\(\times n\)</span> tensor of Tucker rank <span class="math">\((r, r, \ldots, r)\)</span> from Gaussian measurements requires <span class="math">\(\Omega( r n^{K-1} )\)</span> observations. In contrast, a certain (intractable) nonconvex formulation needs only <span class="math">\(O(r^K + nrK)\)</span> observations. We introduce a simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with <span class="math">\(O(r^{\lfloor K/2 \rfloor}n^{\lceil K/2 \rceil})\)</span> observations. The lower bound for the SNN model follows from our new result on recovering signals with multiple structures (e.g. sparse, low rank), which indicates the significant suboptimality of the common approach of minimizing the sum of individual sparsity inducing norms (e.g. <span class="math">\(\ell_1\)</span>, nuclear norm). Our new tractable formulation for low-rank tensor recovery shows how the sample complexity can be reduced by designing convex regularizers that exploit several structures jointly.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="mu14.pdf">Download PDF</a></li>
			
			<li><a href="mu14-supp.zip">Supplementary (ZIP)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
