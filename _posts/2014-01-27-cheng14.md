---
title: Marginalized Denoising Auto-encoders for Nonlinear Representations
abstract: Denoising auto-encoders (DAEs) have been successfully  used to learn new
  representations for a  wide range of machine learning tasks. During  training, DAEs
  make many passes over the training  dataset and reconstruct it from partial corruption  generated
  from a pre-specified corrupting  distribution. This process learns robust representation,  though
  at the expense of requiring many  training epochs, in which the data is explicitly  corrupted.
  In this paper we present the marginalized  Denoising Auto-encoder (mDAE), which  (approximately)
  marginalizes out the corruption  during training. Effectively, the mDAE takes  into
  account infinitely many corrupted copies of  the training data in every epoch, and
  therefore is  able to match or outperform the DAE with much  fewer training epochs.
  We analyze our proposed  algorithm and show that it can be understood as  a classic
  auto-encoder with a special form of regularization.  In empirical evaluations we
  show  that it attains 1-2 order-of-magnitude speedup in  training time over other
  competing approaches.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cheng14
month: 0
firstpage: 1476
lastpage: 1484
page: 1476-1484
sections: 
author:
- given: Minmin
  family: Chen
- given: Kilian
  family: Weinberger
- given: Fei
  family: Sha
- given: Yoshua
  family: Bengio
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/cheng14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
