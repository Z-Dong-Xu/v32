---
supplementary: Supplementary:shamir14-supp.pdf
title: Communication-Efficient Distributed Optimization using an Approximate Newton-type
  Method
abstract: We present a novel Newton-type method for distributed optimization,  which
  is particularly well suited for stochastic optimization and  learning problems.  For
  quadratic objectives, the method enjoys a  linear rate of convergence which provably
  \emphimproves with the  data size, requiring an essentially constant number of iterations  under
  reasonable assumptions.  We provide theoretical and empirical  evidence of the advantages
  of our method compared to other  approaches, such as one-shot parameter averaging
  and ADMM.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shamir14
month: 0
firstpage: 1000
lastpage: 1008
page: 1000-1008
sections: 
author:
- given: Ohad
  family: Shamir
- given: Nati
  family: Srebro
- given: Tong
  family: Zhang
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/shamir14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
