---
supplementary: http://proceedings.mlr.press/v32/kontorovicha14-supp.zip
title: Concentration in unbounded metric spaces and algorithmic stability
abstract: We prove an extension of McDiarmid’s inequality for metric spaces with unbounded
  diameter.  To this end, we introduce the notion of the \em subgaussian diameter,  which
  is a distribution-dependent refinement of the metric diameter.  Our technique provides
  an alternative approach to that of Kutin and Niyogi’s   method of weakly difference-bounded
  functions, and yields nontrivial,   dimension-free results in some interesting cases
  where the former does not.  As an application, we give apparently the first generalization
  bound in the  algorithmic stability setting that holds for unbounded loss functions.  This
  yields a novel risk bound for some regularized metric regression algorithms.  We
  give two extensions of the basic concentration result.  The first enables one to
  replace the independence assumption by appropriate strong mixing.  The second generalizes
  the subgaussian technique to other Orlicz norms.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kontorovicha14
month: 0
tex_title: Concentration in unbounded metric spaces and algorithmic stability
firstpage: 28
lastpage: 36
page: 28-36
order: 28
cycles: false
author:
- given: Aryeh
  family: Kontorovich
date: 2014-06-18
number: 2
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 6
  - 18
pdf: http://proceedings.mlr.press/v32/kontorovicha14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
