<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification">

  <meta name="citation_author" content="Wen, Junfeng">

  <meta name="citation_author" content="Yu, Chun-Nam">

  <meta name="citation_author" content="Greiner, Russell">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="631">
<meta name="citation_lastpage" content="639">
<meta name="citation_pdf_url" content="wen14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Robust Learning under Uncertain Test Distributions: Relating Covariate Shift to Model Misspecification</h1>

	<div id="authors">
	
		Junfeng Wen,
	
		Chun-Nam Yu,
	
		Russell Greiner
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 631â€“639, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Many learning situations involve learning the conditional distribution <span class="math">\(p(y|x)\)</span> when the training instances are drawn from the training distribution <span class="math">\(p_{tr}(x)\)</span>, even though it will later be used to predict for instances drawn from a different test distribution <span class="math">\(p_{te}(x)\)</span>. Most current approaches focus on learning how to reweigh the training examples, to make them resemble the test distribution. However, reweighing does not always help, because (we show that) the test error also depends on the correctness of the underlying model class. This paper analyses this situation by viewing the problem of learning under changing distributions as a game between a learner and an adversary. We characterize when such reweighing is needed, and also provide an algorithm, robust covariate shift adjustment (RCSA), that provides relevant weights. Our empirical studies, on UCI datasets and a real-world cancer prognostic prediction dataset, show that our analysis applies, and that our RCSA works effectively.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="wen14.pdf">Download PDF</a></li>
			
			<li><a href="wen14-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
