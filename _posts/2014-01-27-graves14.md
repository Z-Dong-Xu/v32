---
title: Towards End-To-End Speech Recognition with Recurrent Neural Networks
abstract: This paper presents a speech recognition system that directly transcribes
  audio data with text, without requiring an intermediate phonetic representation.
  The system is based on a combination of the deep bidirectional LSTM recurrent neural
  network architecture and the Connectionist Temporal Classification objective function.
  A modification to the objective function is introduced that trains the network to
  minimise the expectation of an arbitrary transcription loss function. This allows
  a direct optimisation of the word error rate, even in the absence of a lexicon or
  language model. The system achieves a word error rate of 27.3% on the Wall Street
  Journal corpus with no prior linguistic information, 21.9% with only a lexicon of
  allowed words, and 8.2% with a trigram language model. Combining the network with
  a baseline system further reduces the error rate to 6.7%.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: graves14
month: 0
tex_title: Towards End-To-End Speech Recognition with Recurrent Neural Networks
firstpage: 1764
lastpage: 1772
page: 1764-1772
sections: 
author:
- given: Alex
  family: Graves
- given: Navdeep
  family: Jaitly
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/graves14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
