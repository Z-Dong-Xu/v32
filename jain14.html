<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>(Near) Dimension Independent Risk Bounds for Differentially Private Learning | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="(Near) Dimension Independent Risk Bounds for Differentially Private Learning">

  <meta name="citation_author" content="Jain, Prateek">

  <meta name="citation_author" content="Thakurta, Abhradeep Guha">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="476">
<meta name="citation_lastpage" content="484">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v32/jain14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>(Near) Dimension Independent Risk Bounds for Differentially Private Learning</h1>

	<div id="authors">
	
		Prateek Jain,
	
		Abhradeep Guha Thakurta
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 476â€“484, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		In this paper, we study the problem of differentially private risk minimization where the goal is to provide differentially private algorithms that have small excess risk. In particular we address the following open problem: <em>Is it possible to design computationally efficient differentially private risk minimizers with excess risk bounds that do not explicitly depend on dimensionality (<span class="math">\(p\)</span>) and do not require structural assumptions like restricted strong convexity?</em> In this paper, we answer the question in the affirmative for a variant of the well-known <em>output</em> and <em>objective</em> perturbation algorithms [Chaudhuri et al., 2011]. In particular, we show that in generalized linear model, variants of both output and objective perturbation algorithms have no <span><em>explicit</em></span> dependence on <span class="math">\(p\)</span>. Our results assume that the underlying loss function is a <span class="math">\(1\)</span>-Lipschitz convex function and we show that the excess risk depends only on <span class="math">\(L_2\)</span> norm of the true risk minimizer and that of training points. Next, we present a novel privacy preserving algorithm for risk minimization over simplex in the generalized linear model, where the loss function is a doubly differentiable convex function. Assuming that the training points have bounded <span class="math">\(L_\infty\)</span>-norm, our algorithm provides risk bound that has only <span><em>logarithmic</em></span> dependence on <span class="math">\(p\)</span>. We also apply our technique to the online learning setting and obtain a regret bound with similar logarithmic dependence on <span class="math">\(p\)</span>. In contrast, the existing differentially private online learning methods incur <span class="math">\(O(\sqrt{p})\)</span> dependence.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="http://jmlr.org/proceedings/papers/v32/jain14.pdf">Download PDF</a></li>
			
			<li><a href="jain14-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
