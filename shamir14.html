<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Communication-Efficient Distributed Optimization using an Approximate Newton-type Method | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Communication-Efficient Distributed Optimization using an Approximate Newton-type Method">

  <meta name="citation_author" content="Shamir, Ohad">

  <meta name="citation_author" content="Srebro, Nati">

  <meta name="citation_author" content="Zhang, Tong">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="1000">
<meta name="citation_lastpage" content="1008">
<meta name="citation_pdf_url" content="shamir14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Communication-Efficient Distributed Optimization using an Approximate Newton-type Method</h1>

	<div id="authors">
	
		Ohad Shamir,
	
		Nati Srebro,
	
		Tong Zhang
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 1000â€“1008, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably <em>improves</em> with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="shamir14.pdf">Download PDF</a></li>
			
			<li><a href="shamir14-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
