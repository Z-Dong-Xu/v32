<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Input Warping for Bayesian Optimization of Non-Stationary Functions | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Input Warping for Bayesian Optimization of Non-Stationary Functions">

  <meta name="citation_author" content="Snoek, Jasper">

  <meta name="citation_author" content="Swersky, Kevin">

  <meta name="citation_author" content="Zemel, Rich">

  <meta name="citation_author" content="Adams, Ryan">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="1674">
<meta name="citation_lastpage" content="1682">
<meta name="citation_pdf_url" content="snoek14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Input Warping for Bayesian Optimization of Non-Stationary Functions</h1>

	<div id="authors">
	
		Jasper Snoek,
	
		Kevin Swersky,
	
		Rich Zemel,
	
		Ryan Adams
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 1674–1682, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in “log-space”, to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="snoek14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
