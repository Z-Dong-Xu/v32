---
supplementary: Supplementary:brunskill14-supp.pdf
title: PAC-inspired Option Discovery in Lifelong Reinforcement Learning
abstract: A key goal of AI is to create lifelong learning agents that can leverage
  prior experience to improve performance on later tasks. In reinforcement-learning
  problems, one way to summarize prior experience for future use is through options,
  which are temporally extended actions (subpolicies) for how to behave. Options can
  then be used to potentially accelerate learning in new reinforcement learning tasks.
  In this work, we provide the first formal analysis of the sample complexity, a measure
  of learning speed, of reinforcement learning with options.  This analysis helps
  shed light on some interesting  prior empirical results on when and how options
  may accelerate learning. We then quantify the benefit of options in reducing sample
  complexity of a lifelong learning agent. Finally, the new theoretical insights inspire
  a novel option-discovery algorithm that aims at minimizing overall sample complexity
  in lifelong reinforcement learning.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: brunskill14
month: 0
firstpage: 316
lastpage: 324
page: 316-324
sections: 
author:
- given: Emma
  family: Brunskill
- given: Lihong
  family: Li
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/brunskill14/brunskill14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
