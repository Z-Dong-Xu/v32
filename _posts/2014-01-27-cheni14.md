---
supplementary: Supplementary:cheni14-supp.pdf
title: Stochastic Gradient Hamiltonian Monte Carlo
abstract: Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining
  distant proposals with high acceptance probabilities in a Metropolis-Hastings framework,
  enabling more efficient exploration of the state space than standard random-walk
  proposals.  The popularity of such methods has grown significantly in recent years.  However,
  a limitation of HMC methods is the required gradient computation for simulation
  of the Hamiltonian dynamical system-such computation is infeasible in problems involving
  a large sample size or streaming data. Instead, we must rely on a noisy gradient
  estimate computed from a subset of the data.  In this paper, we explore the properties
  of such a stochastic gradient HMC approach. Surprisingly, the natural implementation
  of the stochastic approximation can be arbitrarily bad.  To address this problem
  we introduce a variant that uses second-order Langevin dynamics with a friction
  term that counteracts the effects of the noisy gradient, maintaining the desired
  target distribution as the invariant distribution.  Results on simulated data validate
  our theory.  We also provide an application of our methods to a classification task
  using neural networks and to online Bayesian matrix factorization.
section: cycle-2
layout: inproceedings
id: cheni14
month: 0
firstpage: 1683
lastpage: 1691
page: 1683-1691
sections: 
author:
- given: Tianqi
  family: Chen
- given: Emily
  family: Fox
- given: Carlos
  family: Guestrin
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/cheni14/cheni14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
