---
title: Discrete Chebyshev Classifiers
abstract: In large scale learning problems it is often easy to collect simple statistics
  of the data, but hard or impractical to store all the original data. A key question
  in this setting is how to construct classifiers based on such partial information.
  One traditional approach to the problem has been to use maximum entropy arguments
  to induce a complete distribution on variables from statistics. However, this approach
  essentially makes conditional independence assumptions about the distribution, and
  furthermore does not optimize prediction loss. Here we present a framework for discriminative
  learning given a set of statistics. Specifically, we address the case where all
  variables are discrete and we have access to various marginals.  Our approach minimizes
  the worst case hinge loss in this case, which upper bounds the generalization error.
  We show that for certain sets of statistics the problem is tractable, and in the
  general case can be approximated using MAP LP relaxations. Empirical results show
  that the method is competitive with other approaches that use the same input.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: eban14
month: 0
firstpage: 1233
lastpage: 1241
page: 1233-1241
sections: 
author:
- given: Elad
  family: Eban
- given: Elad
  family: Mezuman
- given: Amir
  family: Globerson
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/eban14/eban14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
