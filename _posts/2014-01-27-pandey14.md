---
supplementary: Supplementary:pandey14-supp.zip
title: Learning by Stretching Deep Networks
abstract: In recent years, deep architectures have gained a lot of prominence for
  learning complex AI tasks  because of their capability to incorporate complex variations
  in data within the model. However, these models often need to be trained for a long
  time in order to obtain good results. In this paper, we propose a technique, called
  ‘stretching’, that allows the same models to perform considerably better with very
  little training.  We show that learning can be done tractably, even when the weight
  matrix is stretched to infinity, for some specific models. We also study tractable
  algorithms for implementing stretching in deep convolutional architectures in an
  iterative manner and derive bounds for its convergence. Our experimental results
  suggest that the proposed stretched deep convolutional networks are capable of achieving
  good performance for many object recognition tasks. More importantly, for a fixed
  network architecture, one can achieve much better accuracy using stretching rather
  than learning the weights using backpropagation.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: pandey14
month: 0
firstpage: 1719
lastpage: 1727
page: 1719-1727
sections: 
author:
- given: Gaurav
  family: Pandey
- given: Ambedkar
  family: Dukkipati
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/pandey14/pandey14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
