<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models | ICML 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models">

  <meta name="citation_author" content="Wang, Jie">

  <meta name="citation_author" content="Li, Qingyang">

  <meta name="citation_author" content="Yang, Sen">

  <meta name="citation_author" content="Fan, Wei">

  <meta name="citation_author" content="Wonka, Peter">

  <meta name="citation_author" content="Ye, Jieping">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of The 31st International Conference on Machine Learning">
<meta name="citation_firstpage" content="235">
<meta name="citation_lastpage" content="243">
<meta name="citation_pdf_url" content="wangb14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>A Highly Scalable Parallel Algorithm for Isotropic Total Variation Models</h1>

	<div id="authors">
	
		Jie Wang,
	
		Qingyang Li,
	
		Sen Yang,
	
		Wei Fan,
	
		Peter Wonka,
	
		Jieping Ye
	<br />
	</div>
	<div id="info">
		Proceedings of The 31st International Conference on Machine Learning,
		pp. 235â€“243, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Total variation (TV) models are among the most popular and successful tools in signal processing. However, due to the complex nature of the TV term, it is challenging to efficiently compute a solution for large-scale problems. State-of-the-art algorithms that are based on the alternating direction method of multipliers (ADMM) often involve solving large-size linear systems. In this paper, we propose a highly scalable parallel algorithm for TV models that is based on a novel decomposition strategy of the problem domain. As a result, the TV models can be decoupled into a set of small and independent subproblems, which admit closed form solutions. This makes our approach particularly suitable for parallel implementation. Our algorithm is guaranteed to converge to its global minimum. With <span class="math">\(N\)</span> variables and n<sub>p</sub> processes, the time complexity is <span class="math">\(O(N/(\epsilon n_p))\)</span> to reach an epsilon-optimal solution. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art algorithms, especially in dealing with high-resolution, mega-size images.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="wangb14.pdf">Download PDF</a></li>
			
			<li><a href="wangb14-supp.zip">Supplementary (ZIP)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
