---
title: An Analysis of State-Relevance Weights and Sampling Distributions on L1-Regularized
  Approximate Linear Programming Approximation Accuracy
abstract: Recent interest in the use of L_1 regularization in the use of value function
  approximation includes Petrik et al.’s introduction of L_1-Regularized Approximate
  Linear Programming (RALP).  RALP is unique among L_1-regularized approaches in that
  it approximates the optimal value function using off-policy samples.  Additionally,
  it produces policies which outperform those of previous methods, such as LSPI.  RALP’s
  value function approximation quality is affected heavily by the choice of state-relevance
  weights in the objective function of the linear program, and by the distribution
  from which samples are drawn; however, there has been no discussion of these considerations
  in the previous literature.  In this paper, we discuss and explain the effects of
  choices in the state-relevance weights and sampling distribution on approximation
  quality, using both theoretical and experimental illustrations.  The results provide
  insight not only onto these effects, but also provide intuition into the types of
  MDPs which are especially well suited for approximation with RALP.
section: cycle-2
layout: inproceedings
id: taylor14
month: 0
firstpage: 451
lastpage: 459
page: 451-459
sections: 
author:
- given: Gavin
  family: Taylor
- given: Connor
  family: Geer
- given: David
  family: Piekut
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/taylor14/taylor14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
