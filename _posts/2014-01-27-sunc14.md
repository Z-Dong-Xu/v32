---
supplementary: Supplementary:sunc14-supp.pdf
title: A Convergence Rate Analysis for LogitBoost, MART and Their Variant
abstract: LogitBoost, MART and their variant can be viewed as additive tree regression
  using logistic loss and boosting style optimization. We analyze their convergence
  rates based on a new weak learnability formulation. We show that it has O(\frac1T)
  rate when using gradient descent only, while a linear rate is achieved when using
  Newton descent. Moreover, introducing Newton descent when growing the trees, as
  LogitBoost does, leads to a faster linear rate. Empirical results on UCI datasets
  support our analysis.
section: cycle-2
layout: inproceedings
id: sunc14
month: 0
firstpage: 1251
lastpage: 1259
page: 1251-1259
sections: 
author:
- given: Peng
  family: Sun
- given: Tong
  family: Zhang
- given: Jie
  family: Zhou
date: 2014-01-27
address: Bejing, China
publisher: PMLR
container-title: Proceedings of The 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 1
  - 27
pdf: http://proceedings.mlr.press/v32/sunc14/sunc14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
