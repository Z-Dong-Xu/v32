---
supplementary: http://proceedings.mlr.press/v32/ahn14-supp.pdf
title: Distributed Stochastic Gradient MCMC
abstract: Probabilistic inference on a big data scale is becoming increasingly relevant
  to both the machine learning and statistics communities. Here we introduce the first
  fully distributed MCMC algorithm based on stochastic gradients. We argue that stochastic
  gradient MCMC algorithms are particularly suited for distributed inference because
  individual chains can draw minibatches from their local pool of data for a flexible
  amount of time before jumping to or syncing with other chains. This greatly reduces
  communication overhead and allows adaptive load balancing. Our experiments for LDA
  on Wikipedia and Pubmed show that relative to the state of the art in distributed
  MCMC we reduce compute time from 27 hours to half an hour in order to reach the
  same perplexity level.
section: cycle-2
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ahn14
month: 0
tex_title: Distributed Stochastic Gradient MCMC
firstpage: 1044
lastpage: 1052
page: 1044-1052
order: 1044
cycles: false
author:
- given: Sungjin
  family: Ahn
- given: Babak
  family: Shahbaba
- given: Max
  family: Welling
date: 2014-06-18
number: 2
address: Bejing, China
publisher: PMLR
container-title: Proceedings of the 31st International Conference on Machine Learning
volume: '32'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 6
  - 18
pdf: http://proceedings.mlr.press/v32/ahn14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
